[
  {
    "objectID": "generative_ml/index.html",
    "href": "generative_ml/index.html",
    "title": "What is a generative model?",
    "section": "",
    "text": "In the most general terms any model that can generate samples from a data distribution is a generative model. Typically we think of image generation and LLMs, but this description covers a huge range of things. We could begin by talking about manual model building, coming up wtih statistical models that try and capture the data generating process of a given observable. There are plenty of good resources on principled ways to build these kinds of statistical models.\nInstead I’ll focus on deep learning and the kinds of models that have driven the spike in google searches for “generative AI”. Just remember that with a generative model we want to be able to generate data that looks similar to our training data. I will provide all the code and models that I use as well, so you can play around with the concepts discussed.\n\nSampling from distributions\nReturning to the original definition, any model that can generate samples, \\(x\\), from a data distribution \\(p(x)\\) is a generative model. Most of the time however \\(p(x)\\) is going to be something complicated like the distribution of all the cat photos in the world. Generative models try and sample from these distributions by first generating samples from a known distribution, \\(z\\), and then learning how to transform them into samples from \\(x\\).\n\n\n\n\n\n\nNote\n\n\n\nLanguage models do something a little different. Text is represented as seqeunce of text segments called tokens. They generate text by learning to predict the probability of \\(x\\) being the next token conditioned on the previous tokens in the sequence \\(p(x_i | x_{_i-1}...)\\).\n\n\nThis might sound mysterious but the same idea can be used to generate samples from arbitrarty statistical distributions. The method, called inverse transform sampling, works by taking a sample, \\(z\\), from a uniform distribution \\(\\text{U}(0, 1)\\) and then applying a transformation \\(p(z)\\) so that \\[\nx = p(z).\n\\] In this case \\(p(z)\\) is the inverse cumalative distribution function of \\(x\\). The first generative model that we’ll dicuss does the same thing by replacing \\(x\\) with images and \\(p(z)\\) with a neural network."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Fill this in when you have time"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Machine Learning blog",
    "section": "",
    "text": "A blog filled with side projects and things I want to learn. having a place to put things is mainly a way to fore me to be more structured in what I do"
  },
  {
    "objectID": "generative_ml/vae_1.html",
    "href": "generative_ml/vae_1.html",
    "title": "1. VAEs: Variational autoencoders",
    "section": "",
    "text": "If you’re familiar with building and training neural networks autoencoders are a good place to start since they’ll demonstrate why the idea of being able to sample from a distribution is important. The way they work is also pretty intuitive. As we progress it will become clear why they’re not suitable for generative modelling and the changes we need to make to bridge the gap. Autoencoders learn low dimensional representations by passing data through an information bottleneck and try to reconstruct it at the other end.\n\n\nFor this example we’ll try and generate some faces from the famous CelebA dataset. The dataset contains more than 200 thousand images of celebrities. The images have also been cropped so that faces are centred, which makes the machine learning task a bit easier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Examples from CelebA validation set\n\n\nThe dataset also comes with a set of attributes labels for each image for things like “Blond Hair” and “Eyeglasses”. These will be useful when we want to explore the encoded representations of the images that we generate.\n\n\n\nYou can break down an autoencoder network into two parts, the encoder and decoder. It works by passing data through the encoder, which takes the input and squeezes it down to vector that is smaller in size. This is the low dimensional bottleneck. This vector is then input to the decoder which tries to reconstruct the input image. The model is trained by minimizing the difference (more on this later) between the input image and reconstructed image. In order for this to work The output of the encoder needs to contain enough information about the input data so that the decoder network can recreate it.\n\n\n\nAutoencoder structure\n\n\nThe encoder output can be considered a low(er) dimensional vector representation of the input data. They’re often called latent or hidden vectors since they’re supposed to contain the latent attributes of the data. For our purposes though the most interesting part is the decoder. If you train an autoencoder on a collection of images then you should be able to generate new images by passing simple, low dimensional data into the decoder input!\n\n\n\nWe can build the encoder and decoder layers using a series of convolutional (and transpose convolutional) layers. This is a common strategy for image based ML problems and, since there are other excellent resources describing how convolutional layers work, including the link to pytorch docs, I won’t discuss them here. All the code written for this example can be found here. In building the model for this example I took plenty of inspiration from Ref to deep learning book and the repo.\nThe nice thing is that we can abstract away most of the complexity when it comes to training. The model, with an input sample \\(x\\) can be represented simply as \\[\n\\hat{x} = g(f(x))\n\\] where \\(\\hat{x}\\) is the reconstructed sample and \\(g\\) and \\(f\\) are the decoder and encoder respectively. The latent vector \\(h\\) is given by \\[\nh = f(x).\n\\] Finally, training the autoencoder means simply picking a loss function \\(L\\) and minimising \\[\nL(x, g(f(x))).\n\\] All we need to do now is select a loss function. In this case \\(x\\) is a tensor of RGB pixel intensities for each image. The values are scaled to be between 0 and 1. Using the mean squared error (MSE) between our reconstructed and original pixel values is a reasonable choice, forcing the model to try and get the correct pixel values.\n\n\n\n\n\n\nNote\n\n\n\nUsing MSE loss in image reconstruction tasks can make the model less sensitive to small amounts of gaussian blur around the target pixel values, which makes images look blurry.\n\n\nKeeping things simple and training the model using the Adam optimiser for 10 epochs achieves reasonable results. Practically no time was spent fine tuning this model but it does pretty well. Training took about 30 minutes on a single GPU. Instead of plotting training loss curves it’s more insightful to just plot the input and output images directly.\n\n\nA side by side comparison of input and output images of the autoencoder should give some sense of how well it’s performing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Original images\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Autoencoder output\n\n\nA side by side comparison shows that while a lot of detail is lost the reconstructed images do resemble the originals. At the very least they look like human faces! What’s interesting is that it’s not just blurry, some of the fine details are lost. Subtle things like where the eyes are pointing and the asymmetry of a smile. Some things like jewellery and ties are lost completely. This happens because that information doesn’t make it though the bottleneck, probably since it’s less important than other facial characteristics when it comes to reconstructing pixel values. When information is missing the decoder just substitues it with something that “sort of works” for most training examples, or misses it entirely. Its definitely possible to do better at this stage but the decoder does seem to be able to generate faces.\n\n\n\n\n\n\nMaking the models public\n\n\n\nIf there is interest I can make the pre-trained decoder available so that you can try an generate images with it yourself.\n\n\n\n\n\n\nThis is where things get tricky. It should be possible to generate new faces by creating new latent vectors and passing them through the decoder. What isn’t clear however is how to generate the latent vectors. The problem of generating faces has now turned into the problem of generating latent vectors. This is the issue that VAEs address and is a good way to understand the underlying assumptions.\nBefore we explore that however, we can see what happens if we try and make faces by taking an educated guess at what reasonable latent vectors look like. Remember, the goal is generate faces that are realistic and human looking, but distinct from those that are in the dataset. We can try and be clever and encode a set of images from the test set, take the mean and standard deviation values of the latent vectors generated and, in a bit of a hand-wavey attempt to generate vectors from the same distribution as the encoded sample, generate new vectors by sampling from a normal distribution with the same parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Passing random latent vectors through the decoder generates these images.\n\n\nTurns out this doesn’t work so well. If you squint at some of these image you can just about see a tortured face peering out from under the random blobs of colour, but nothing like the image that were generated above. This is the problem with using models like autoencoders to generate new data. There are no constraints on what values the latent vectors take, so when it comes to trying to generate new images you’re stuck.\n\n\n\n\n\n\nNote\n\n\n\nGoing a step further and generating vectors by using the mean and standard deviation of each dimension independetly generates slightly more realistic looking images, but similar problems are observed."
  },
  {
    "objectID": "generative_ml/vae_1.html#defining-a-dataset",
    "href": "generative_ml/vae_1.html#defining-a-dataset",
    "title": "1. VAEs: Variational autoencoders",
    "section": "",
    "text": "For this example we’ll try and generate some faces from the famous CelebA dataset. The dataset contains more than 200 thousand images of celebrities. The images have also been cropped so that faces are centred, which makes the machine learning task a bit easier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Examples from CelebA validation set\n\n\nThe dataset also comes with a set of attributes labels for each image for things like “Blond Hair” and “Eyeglasses”. These will be useful when we want to explore the encoded representations of the images that we generate."
  },
  {
    "objectID": "generative_ml/vae_1.html#what-is-an-autoencoder",
    "href": "generative_ml/vae_1.html#what-is-an-autoencoder",
    "title": "1. VAEs: Variational autoencoders",
    "section": "",
    "text": "You can break down an autoencoder network into two parts, the encoder and decoder. It works by passing data through the encoder, which takes the input and squeezes it down to vector that is smaller in size. This is the low dimensional bottleneck. This vector is then input to the decoder which tries to reconstruct the input image. The model is trained by minimizing the difference (more on this later) between the input image and reconstructed image. In order for this to work The output of the encoder needs to contain enough information about the input data so that the decoder network can recreate it.\n\n\n\nAutoencoder structure\n\n\nThe encoder output can be considered a low(er) dimensional vector representation of the input data. They’re often called latent or hidden vectors since they’re supposed to contain the latent attributes of the data. For our purposes though the most interesting part is the decoder. If you train an autoencoder on a collection of images then you should be able to generate new images by passing simple, low dimensional data into the decoder input!"
  },
  {
    "objectID": "generative_ml/vae_1.html#model-structure-and-training",
    "href": "generative_ml/vae_1.html#model-structure-and-training",
    "title": "1. VAEs: Variational autoencoders",
    "section": "",
    "text": "We can build the encoder and decoder layers using a series of convolutional (and transpose convolutional) layers. This is a common strategy for image based ML problems and, since there are other excellent resources describing how convolutional layers work, including the link to pytorch docs, I won’t discuss them here. All the code written for this example can be found here. In building the model for this example I took plenty of inspiration from Ref to deep learning book and the repo.\nThe nice thing is that we can abstract away most of the complexity when it comes to training. The model, with an input sample \\(x\\) can be represented simply as \\[\n\\hat{x} = g(f(x))\n\\] where \\(\\hat{x}\\) is the reconstructed sample and \\(g\\) and \\(f\\) are the decoder and encoder respectively. The latent vector \\(h\\) is given by \\[\nh = f(x).\n\\] Finally, training the autoencoder means simply picking a loss function \\(L\\) and minimising \\[\nL(x, g(f(x))).\n\\] All we need to do now is select a loss function. In this case \\(x\\) is a tensor of RGB pixel intensities for each image. The values are scaled to be between 0 and 1. Using the mean squared error (MSE) between our reconstructed and original pixel values is a reasonable choice, forcing the model to try and get the correct pixel values.\n\n\n\n\n\n\nNote\n\n\n\nUsing MSE loss in image reconstruction tasks can make the model less sensitive to small amounts of gaussian blur around the target pixel values, which makes images look blurry.\n\n\nKeeping things simple and training the model using the Adam optimiser for 10 epochs achieves reasonable results. Practically no time was spent fine tuning this model but it does pretty well. Training took about 30 minutes on a single GPU. Instead of plotting training loss curves it’s more insightful to just plot the input and output images directly.\n\n\nA side by side comparison of input and output images of the autoencoder should give some sense of how well it’s performing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Original images\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Autoencoder output\n\n\nA side by side comparison shows that while a lot of detail is lost the reconstructed images do resemble the originals. At the very least they look like human faces! What’s interesting is that it’s not just blurry, some of the fine details are lost. Subtle things like where the eyes are pointing and the asymmetry of a smile. Some things like jewellery and ties are lost completely. This happens because that information doesn’t make it though the bottleneck, probably since it’s less important than other facial characteristics when it comes to reconstructing pixel values. When information is missing the decoder just substitues it with something that “sort of works” for most training examples, or misses it entirely. Its definitely possible to do better at this stage but the decoder does seem to be able to generate faces.\n\n\n\n\n\n\nMaking the models public\n\n\n\nIf there is interest I can make the pre-trained decoder available so that you can try an generate images with it yourself."
  },
  {
    "objectID": "generative_ml/vae_1.html#generating-new-images",
    "href": "generative_ml/vae_1.html#generating-new-images",
    "title": "1. VAEs: Variational autoencoders",
    "section": "",
    "text": "This is where things get tricky. It should be possible to generate new faces by creating new latent vectors and passing them through the decoder. What isn’t clear however is how to generate the latent vectors. The problem of generating faces has now turned into the problem of generating latent vectors. This is the issue that VAEs address and is a good way to understand the underlying assumptions.\nBefore we explore that however, we can see what happens if we try and make faces by taking an educated guess at what reasonable latent vectors look like. Remember, the goal is generate faces that are realistic and human looking, but distinct from those that are in the dataset. We can try and be clever and encode a set of images from the test set, take the mean and standard deviation values of the latent vectors generated and, in a bit of a hand-wavey attempt to generate vectors from the same distribution as the encoded sample, generate new vectors by sampling from a normal distribution with the same parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Passing random latent vectors through the decoder generates these images.\n\n\nTurns out this doesn’t work so well. If you squint at some of these image you can just about see a tortured face peering out from under the random blobs of colour, but nothing like the image that were generated above. This is the problem with using models like autoencoders to generate new data. There are no constraints on what values the latent vectors take, so when it comes to trying to generate new images you’re stuck.\n\n\n\n\n\n\nNote\n\n\n\nGoing a step further and generating vectors by using the mean and standard deviation of each dimension independetly generates slightly more realistic looking images, but similar problems are observed."
  }
]