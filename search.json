[
  {
    "objectID": "generative_ml/vae_1.html",
    "href": "generative_ml/vae_1.html",
    "title": "VAEs: Variational autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) were introduced by Kingma et al as an efficient method to learn posterior distributions using gradient descent. The paper describes how neural networks can be used to efficiently create models that can generate complex data by sampling from simple, known distributions. For those who are new to generative modelling contrasting VAEs with autoencoders can help highlight the key ideas."
  },
  {
    "objectID": "generative_ml/vae_1.html#defining-a-dataset",
    "href": "generative_ml/vae_1.html#defining-a-dataset",
    "title": "VAEs: Variational autoencoders",
    "section": "Defining a dataset",
    "text": "Defining a dataset\nFor this example we’ll try and generate some faces from the famous CelebA dataset. The dataset contains more than 200 thousand images of celebrities. The images have also been cropped so that faces are centred, which makes the machine learning task a bit easier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Examples from CelebA validation set\n\n\nThe dataset also comes with a set of attributes labels for each image for things like “Blond Hair” and “Eyeglasses”. These will be useful when we want to explore the encoded representations of the images that we generate."
  },
  {
    "objectID": "generative_ml/vae_1.html#what-is-an-autoencoder",
    "href": "generative_ml/vae_1.html#what-is-an-autoencoder",
    "title": "VAEs: Variational autoencoders",
    "section": "What is an autoencoder?",
    "text": "What is an autoencoder?\nYou can break down an autoencoder network into two parts, the encoder and decoder. It works by passing data through the encoder, which takes the input and squeezes it down to vector that is smaller in size. This is the low dimensional bottleneck. This vector is then input to the decoder which tries to reconstruct the input image. The model is trained by minimizing the difference (more on this later) between the input image and reconstructed image. In order for this to work The output of the encoder needs to contain enough information about the input data so that the decoder network can recreate it.\n\n\n\nAutoencoder structure\n\n\nThe encoder output can be considered a low(er) dimensional vector representation of the input data. They’re often called latent or hidden vectors since they’re supposed to contain the latent attributes of the data. For our purposes though the most interesting part is the decoder. If you train an autoencoder on a collection of images then you should be able to generate new images by passing simple, low dimensional data into the decoder input!"
  },
  {
    "objectID": "generative_ml/vae_1.html#model-structure-and-training",
    "href": "generative_ml/vae_1.html#model-structure-and-training",
    "title": "VAEs: Variational autoencoders",
    "section": "Model structure and training",
    "text": "Model structure and training\nWe can build the encoder and decoder layers using a series of convolutional (and transpose convolutional) layers. This is a common strategy for image based ML problems and, since there are other excellent resources describing how convolutional layers work, including the link to pytorch docs, I won’t discuss them here. All the code written for this example can be found here. In building the model for this example I took plenty of inspiration from Ref to deep learning book and the repo.\nThe nice thing is that we can abstract away most of the complexity when it comes to training. The model, with an input sample \\(x\\) can be represented simply as \\[\n\\hat{x} = g(f(x))\n\\] where \\(\\hat{x}\\) is the reconstructed sample and \\(g\\) and \\(f\\) are the decoder and encoder respectively. The latent vector \\(h\\) is given by \\[\nh = f(x).\n\\] Finally, training the autoencoder means simply picking a loss function \\(L\\) and minimising \\[\nL(x, g(f(x))).\n\\] All we need to do now is select a loss function. In this case \\(x\\) is a tensor of RGB pixel intensities for each image. The values are scaled to be between 0 and 1. Using the mean squared error (MSE) between our reconstructed and original pixel values is a reasonable choice, forcing the model to try and get the correct pixel values.\n\n\n\n\n\n\nChoice of loss function\n\n\n\nUsing MSE loss in image reconstruction tasks can make the model less sensitive to small amounts of gaussian blur around the target pixel values, which makes images look blurry.\n\n\nKeeping things simple and training the model using the Adam optimiser for 10 epochs achieves reasonable results. Practically no time was spent fine tuning this model but it does pretty well. Training took about 30 minutes on a single GPU. Instead of plotting training loss curves it’s more insightful to just plot the input and output images directly.\n\nComparing inputs and outputs\nA side by side comparison of input and output images of the autoencoder should give some sense of how well it’s performing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Original images\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Autoencoder output\n\n\nA side by side comparison shows that while a lot of detail is lost the reconstructed images do resemble the originals. At the very least they look like human faces! What’s interesting is that it’s not just blurry, some of the fine details are lost. Subtle things like where the eyes are pointing and the asymmetry of a smile. Some things like jewellery and ties are lost completely. This happens because that information doesn’t make it though the bottleneck, probably since it’s less important than other facial characteristics when it comes to reconstructing pixel values. When information is missing the decoder just substitues it with something that “sort of works” for most training examples, or misses it entirely. Its definitely possible to do better at this stage but the decoder does seem to be able to generate faces.\n\n\n\n\n\n\nMaking the models public\n\n\n\nIf there is interest I can make the pre-trained decoder available so that you can try an generate images with it yourself."
  },
  {
    "objectID": "generative_ml/vae_1.html#generating-new-images",
    "href": "generative_ml/vae_1.html#generating-new-images",
    "title": "VAEs: Variational autoencoders",
    "section": "Generating (new) images",
    "text": "Generating (new) images\nThis is where things get tricky. It should be possible to generate new faces by creating new latent vectors and passing them through the decoder. What isn’t clear however is how to generate the latent vectors. The problem of generating faces has now turned into the problem of generating latent vectors. This is the issue that VAEs address and is a good way to understand the underlying assumptions.\nBefore we explore that however, we can see what happens if we try and make faces by taking an educated guess at what reasonable latent vectors look like. Remember, the goal is generate faces that are realistic and human looking, but distinct from those that are in the dataset. We can try and be clever and encode a set of images from the test set, take the mean and standard deviation values of the latent vectors generated and, in a bit of a hand-wavey attempt to generate vectors from the same distribution as the encoded sample, generate new vectors by sampling from a normal distribution with the same parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Passing random latent vectors through the decoder generates these images.\n\n\nTurns out this doesn’t work so well. If you squint at some of these image you can just about see a tortured face peering out from under the random blobs of colour, but nothing like the image that were generated above. This is the problem with using models like autoencoders to generate new data. There are no constraints on what values the latent vectors take, so when it comes to trying to generate new images you’re stuck.\n\n\n\n\n\n\nNote\n\n\n\nGoing a step further and generating vectors by using the mean and standard deviation of each dimension independently generates slightly more realistic looking images, but similar problems are observed."
  },
  {
    "objectID": "generative_ml/vae_1.html#thinking-in-terms-of-distributions",
    "href": "generative_ml/vae_1.html#thinking-in-terms-of-distributions",
    "title": "VAEs: Variational autoencoders",
    "section": "Thinking in terms of distributions",
    "text": "Thinking in terms of distributions\nInstead of optimising encoder and decoder functions consider a VAE as a latent variable model. VAEs address the problem of sampling the latent vectors directly by constraining them to be drawn from a known distribution, modelling the join probability of the latent variables \\(z\\) and the data \\(x\\). This is the equivalent of putting a prior on the \\(z\\) vectors in a Bayesian framework. Typically we choose this distribution to be a isotropic multidimensional Gaussian, assuming no correlation between any of the dimensions of the latent vector: \\[\nP(z) \\sim \\mathcal{N}(0, I).\n\\] The choice of distribution is just an assumptions but has some useful properties that we’ll see later. This change in modelling assumptions has a significant impact on how VAEs are trained and it becomes clear that they’re actually only superficially similar to autoencoders.\n\n\n\n\n\n\nWhy a Gaussian?\n\n\n\n\n\nA natural question to ask is whether the choice of prior distribution constrains the model outputs in anyway. The good news is that it doesn’t, any \\(d\\) dimensional distribution can be generated by sampling \\(d\\) variables from a normal distribution and transforming them with a sufficiently complex function. This is how inverse-transform sampling works, as discussed on this page.\n\n\n\nWe can write down the model as the join probability of the latent vectors and the data, \\(P(x, z)\\). Using the chain rule of probability we can write this out in a way that better defines the generative process: \\[\nP(x, z) = P(x | z)P(z)\n\\] where the process is\n\nSample a latent vector \\(z \\sim P(z)\\)\nGenerate a data point \\(x \\sim P(x | z)\\).\n\nIt is helpful to rewrite \\(P(x | z)\\) as \\(P(x | z; \\theta)\\), where \\(\\theta\\) represents the parameters of the distribution \\(P_{x|z}\\). What we want to optimise (maximise in this case), with respect to \\(\\theta\\), is the probability of each training example \\(x\\) under the generative process \\[\n\\begin{aligned}\nP(x) & = \\int P(x | z; \\theta)P(z) dz. \\\\\n& = \\mathbb{E}_{z \\sim P(z)}P(x | z; \\theta)\n\\end{aligned}\n\\] We have already chosen how to define \\(P(z)\\) but we still need to compute the integral over \\(z\\). There are some key issues that make this quite difficult.\n\n\n\n\n\n\nThe Kullback–Leibler (KL) divergence\n\n\n\nThe KL divergence is defined as \\[\n\\begin{aligned}\n\\mathcal{D}(P \\parallel Q) &= \\int p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right) dx \\\\\n&= \\mathbb{E}_{p(x)} \\left[ \\frac{p(x)}{q(x)} \\right]\n\\end{aligned}\n\\] and is used as a measure to describe how one probability distribution differs from another."
  },
  {
    "objectID": "generative_ml/vae_1.html#what-is-a-vae",
    "href": "generative_ml/vae_1.html#what-is-a-vae",
    "title": "VAEs: Variational autoencoders",
    "section": "What is a VAE?",
    "text": "What is a VAE?\nIn principle we can maximise \\(P(x)\\) directly by sampling \\(n\\) values of \\(z\\) and then computing \\(P(x) \\approx \\frac{1}{n} \\sum_i P(x | z_i; \\theta)\\). The problem here is just how many samples it would take. If you consider all the possible configurations of the pixels in the images \\(x\\) its obvious that for the vast majority of the time \\(P(x|z; \\theta)\\) will be almost zero and sampling the corresponding \\(z\\) values will not help improve our estimates of \\(P(x|z; \\theta)\\). This makes using traditional inference techniques such as Markov Chain Monte Carlo (MCMC) difficult, even before you consider the number of parameters. The discussion in this excellent introduction goes into more detail.\nWhat makes VAEs more efficient is that during training we try and sample values of \\(z\\) that are likely to produce \\(x\\). This is done by introducing a new function \\(Q( z | x)\\), which takes a value of \\(x\\) and returns a distribution over \\(z\\) values which are likely to produce \\(x\\). Where the generative model we’ve describe is analogous to the decoder part of the autoencoder this part is analogous to the encoder.\n\n\n\n\n\n\nNotation\n\n\n\nThere is a bit of notation overload here, which I have coped from Doersch. I’m using P(…) to sometimes refer to generic probability distributions and sometimes specific functions. Hopefully with context its pretty clear in this example.\n\n\nSince we now sample \\(z \\sim Q(z | x)\\) our definition for \\(P(x)\\) becomes \\[\n\\begin{aligned}\nP(x) &= \\int P(x | z; \\theta)Q(z | x) dz \\\\\n&= \\mathbb{E}_{z \\sim Q}P(x | z; \\theta).\n\\end{aligned}\n\\] Our goal is now to re-write this in terms of an expression which we can evaluate easily and therefore maximise. We do this by defining the ELBO (evidence lower bound) and showing that maximising the ELBO also maximises \\(P(x)\\). (Or, \\(\\ln P(x)\\), since the logarithm is a monotonic function.)\n\nDeriving the ELBO\nStarting from the definition of \\(P(x)\\) and taking the natural logarithm,\n\\[\\begin{aligned}\n\\ln P(x) &= \\ln \\left[  \\mathbb{E}_{z \\sim Q} P(x)\\right] = \\mathbb{E}_{z \\sim Q} \\ln  P(x)  &(P(x)~\\text{is independent of}~z) \\\\\n\n& =  \\mathbb{E}_{z \\sim Q} \\left[ \\ln \\frac{P(x | z)P(z)}{P(z | x)} \\right] & (\\text{Bayes' rule}) \\\\\n\n& = \\mathbb{E}_{z \\sim Q} \\left[ \\ln \\frac{P(x | z)P(z)}{P(z | x)} \\frac{Q(z | x)}{Q(z | x)} \\right] & (\\text{multiply by}~1) \\\\\n\n& = \\mathbb{E}_{z \\sim Q} \\ln P(x | z)\n- \\mathbb{E}_{z \\sim Q} \\left[ \\ln \\frac{Q(z | x)}{P(z)} \\right]  \n+ \\mathbb{E}_{z \\sim Q} \\left[ \\ln \\frac{Q(z | x)}{P(z | x)} \\right] \\\\\\\\\n\n\\ln P(x) &= \\mathbb{E}_{z \\sim Q} \\ln P(x | z) - \\mathcal{D}(Q(z | x) \\parallel P(z))\n         +\n\\underbrace{\\mathcal{D}(Q(z | x) \\parallel P(z | x))}_{\\text{intractable},~\\geq 0}.\n\\end{aligned}\\]\nIt’s possible to express the quantity we want to maximise in terms of several terms one of which is The KL divergence of \\(Q\\) with an intractable term \\(P(z | x)\\). The critical thing consider here is that the KL divergence of any two distributions is always \\(\\geq 0\\). This allows us to write\n\\[\\begin{aligned}\n\\ln P(x) &\\geq \\underbrace{\n    \\underbrace{\n        \\mathbb{E}_{z \\sim Q} \\ln P(x | z)\n        }_\\text{reconstruction error}\n        -\n        \\underbrace{\n            \\mathcal{D}(Q(z | x) \\parallel P(z)).\n        }_\\text{fixes the latent}\n}_\\text{ELBO}\n\\end{aligned}\\]\nThe ELBO consists of two terms\n\nOne that represents the reconstruction error\nThe (negative) KL divergence between \\(Q\\) and the desired distribution of \\(z\\), \\(P(z)\\). This adds a larger penalty for versions of \\(Q\\) that generate \\(z\\) distributions that deviate from \\(P(z)\\).\n\nSince \\(P(z)\\) is bound to be greater or equal to the ELBO, maximising the ELBO maximises the quantity that we care about.\n\n\n\n\n\n\nAlternate derivations\n\n\n\n\n\nThere are several methods to derive the ELBO. One method relies on Jensen’s inequality while another starts with KL divergence between \\(Q(z|x)\\) and the unknown distribution \\(P(z|x)\\). The method I’ve chosen starts from the definition of \\(P(x)\\) given above."
  },
  {
    "objectID": "generative_ml/vae_1.html#building-and-training-vae",
    "href": "generative_ml/vae_1.html#building-and-training-vae",
    "title": "VAEs: Variational autoencoders",
    "section": "Building and training VAE",
    "text": "Building and training VAE\nNow that we’ve developed a theoretical understanding of that a VAE is, it should be possible to map the autoencoder architecture we’ve used already into one. The encoder does the job of \\(Q(z | x)\\), mapping images to latent vectors. The main difference however is that the encoder should generate a distribution of \\(z\\) given \\(x\\) rather than a scalar vector. The decoder in turn represents \\(P(x | z).\\) This introduces a new challenge. How can we sample from a parametrised distribution while training a neural network via gradient descent? We need to be able to calculate the gradient of the loss with respect to all of the trainable parameters in the network.\n\nThe reparametrisation trick\nTurns out that there is an elegant solution. The reparametrisation trick separates the stochastic part from the trainable parameters of the distribution. The encoder needs to generate samples drawn from a multivariate Gaussian distribution, parametrised by mean vector \\(\\mu\\) and the variance \\(\\sigma\\). This can be done by modifying the autoencoder encoder to output two vectors, one that represents the mean \\(\\mu\\) and one for (the log of) the variance \\(\\sigma\\). The stochastic encoder output is then created by sampling a vector from the standard normal distribution and then shifting and scaling appropriately \\[\nQ(z | x) = \\mu + ( \\mathcal{N}(0, 1) \\cdot \\sigma  ).\n\\] During training we can therefore still calculate gradients with respect to \\(\\mu\\) and \\(\\sigma\\). We could sample values probabilistically for the pixel densities but this isn’t necessary in order to calculate the loss and typically isn’t done.\n\n\ndefining the loss function\nThe first term of the loss function, based on the ELBO, represents the likelihood of generating the target image \\(x\\) and can be interpreted as the reconstruction error. In the first example we used the MSE loss for our reconstruction error. Under the assumption that the pixel values are drawn from normal distributions the MSE loss can be interpreted as a likelihood, so that part can remain the same. Defining the second term can be defined relatively easily since we’ve fixed both \\(P(z)\\) and \\(Q(z | x)\\) to be a multivariate Guassians. The loss function then becomes\n\n\n\n\n\n\nThe \\(\\beta\\)-VAE\n\n\n\n\n\nblah\n\n\n\n\n\n\nThe distribution of latent dimension vectors as encoded from the validation set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Passing random latent vectors through the VAE generates somewhat realistic looking faces.\n\n\n% mention this after adding something about P(z|x) It might be a good time to pause here before we discuss how we can optimise this and think about why this formulation helps us generate new data. During training\nNot only will be now be able to sample latent vectors from\n\n\n\n\n\n\nA PyTorch example\n\n\n\n\n\nIt’s possible to make relatively small modifications to an autoencoder implemented in PyTorch and turn it into a VAE.\nclass VAE(AutoEncoder):\n    \"\"\"construct a VAE by using the basic same skeleton\n    as the AutoEncoder, but by adding and overwriting a few things\n    \"\"\"\n\n    def __init__(\n        self,\n        latent_dim,\n        input_channels=3,\n        encoder_channel_out_sizes=[16, 32, 64, 128, 256],\n        input_image_h_w=(218, 178),\n    ):\n        super().__init__(\n            latent_dim,\n            input_channels,\n            encoder_channel_out_sizes,\n            input_image_h_w,\n        )\n        # Add another linear layer for the log variance\n        self.encoder_linear_logvar = nn.Linear(self.linear_layer_size, self.latent_dim)\n\n    @staticmethod\n    def reparameterise(mu, logvar):\n        \"\"\"Implement the reparameterisation trick\"\"\"\n        # scale the log variance to std\n        std = torch.exp(0.5 * logvar)\n        # sample from a standard noem\n        normal_noise = torch.randn_like(std)\n        return mu + (std * normal_noise)\n\n    def encode(self, input: torch.Tensor):\n        \"\"\"Helper function making it easier to encode\n        images\n        \"\"\"\n        x = self.encoder(input)\n        mu = self.encoder_linear_mean(x)\n        logvar = self.encoder_linear_logvar(x)\n        return self.reparameterise(mu, logvar), mu, logvar\n\n    def forward(self, input):\n        encoded, mu, logvar = self.encode(input)\n        return self.decode(encoded), mu, logvar\n\n    def loss_function(self, input, target, mu, logvar):\n        # summed over each element in the latent space\n        # the mean is taken over training examples\n        kld_loss = torch.mean(\n            -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp(), dim=1), dim=0\n        )\n        kld_loss = kld_loss * 1e-5\n\n        reconstruction_loss = nn.functional.mse_loss(input, target)\n\n        return (\n            reconstruction_loss + kld_loss,\n            reconstruction_loss.detach(),\n            kld_loss.detach(),\n        )\n\n\n\nIntuitively\n\nI remember asking a researcher at my old job exactly why VAEs made the assumption that elements in the latent space were normally distributed and the response was “that’s just the assumption of the model. At the time it did not help.\n\n\nReferences"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name’s Amal and I’m currently working as a ML scientist at J. P. Morgan in London. My work currently involves working with LLMs, thinking about efficient ways to fine-tune them and detect hallucinations in specific scenarios. I have a broad range of interests in the ML space including Bayesian inference, timeseries and generative modelling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Machine Learning blog",
    "section": "",
    "text": "Hi! My name’s Amal\n\nI’m currently working as a ML scientist at J. P. Morgan in London. My work currently involves working with LLMs, thinking about efficient ways to fine-tune them and detect hallucinations in specific scenarios. I have a broad range of interests in the ML space including Bayesian inference, timeseries and generative modelling."
  },
  {
    "objectID": "generative_ml/index.html",
    "href": "generative_ml/index.html",
    "title": "What is a generative model?",
    "section": "",
    "text": "In the most general terms any model that can generate samples from a data distribution is a generative model. Typically we think of image generation and LLMs, but this description covers a huge range of things. We could begin by talking about manual model building, coming up wtih statistical models that try and capture the data generating process of a given observable. There are plenty of good resources on principled ways to build these kinds of statistical models.\nInstead I’ll focus on deep learning and the kinds of models that have driven the spike in google searches for “generative AI”. Just remember that with a generative model we want to be able to generate data that looks similar to our training data. I will provide all the code and models that I use as well, so you can play around with the concepts discussed.\n\nSampling from distributions\nReturning to the original definition, any model that can generate samples, \\(x\\), from a data distribution \\(p(x)\\) is a generative model. Most of the time however \\(p(x)\\) is going to be something complicated like the distribution of all the cat photos in the world. Generative models try and sample from these distributions by first generating samples from a known distribution, \\(z\\), and then learning how to transform them into samples from \\(x\\).\n\n\n\n\n\n\nNote\n\n\n\nLanguage models do something a little different. Text is represented as seqeunce of text segments called tokens. They generate text by learning to predict the probability of \\(x\\) being the next token conditioned on the previous tokens in the sequence \\(p(x_i | x_{_i-1}...)\\).\n\n\nThis might sound mysterious but the same idea can be used to generate samples from arbitrarty statistical distributions. The method, called inverse transform sampling, works by taking a sample, \\(z\\), from a uniform distribution \\(\\text{U}(0, 1)\\) and then applying a transformation \\(p(z)\\) so that \\[\nx = p(z).\n\\] In this case \\(p(z)\\) is the inverse cumalative distribution function of \\(x\\). The first generative model that we’ll dicuss does the same thing by replacing \\(x\\) with images and \\(p(z)\\) with a neural network."
  }
]