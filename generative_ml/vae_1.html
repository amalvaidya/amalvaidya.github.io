<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Intrinsic order - VAEs: Variational autoencoders</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Intrinsic order</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../generative_ml/index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Generative AI</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../generative_ml/vae_1.html">VAEs: Variational autoencoders</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../generative_ml/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is a generative model?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../generative_ml/vae_1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">VAEs: Variational autoencoders</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../generative_ml/diffusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diffusion models</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#starting-simple-with-autoencoders" id="toc-starting-simple-with-autoencoders" class="nav-link active" data-scroll-target="#starting-simple-with-autoencoders">Starting simple with autoencoders</a>
  <ul class="collapse">
  <li><a href="#defining-a-dataset" id="toc-defining-a-dataset" class="nav-link" data-scroll-target="#defining-a-dataset">Defining a dataset</a></li>
  <li><a href="#what-is-an-autoencoder" id="toc-what-is-an-autoencoder" class="nav-link" data-scroll-target="#what-is-an-autoencoder">What is an autoencoder?</a></li>
  <li><a href="#model-structure-and-training" id="toc-model-structure-and-training" class="nav-link" data-scroll-target="#model-structure-and-training">Model structure and training</a></li>
  <li><a href="#generating-new-images" id="toc-generating-new-images" class="nav-link" data-scroll-target="#generating-new-images">Generating (new) images</a></li>
  </ul></li>
  <li><a href="#vaes-to-the-rescue" id="toc-vaes-to-the-rescue" class="nav-link" data-scroll-target="#vaes-to-the-rescue">VAEs to the rescue</a>
  <ul class="collapse">
  <li><a href="#thinking-in-terms-of-distributions" id="toc-thinking-in-terms-of-distributions" class="nav-link" data-scroll-target="#thinking-in-terms-of-distributions">Thinking in terms of distributions</a></li>
  <li><a href="#what-is-a-vae" id="toc-what-is-a-vae" class="nav-link" data-scroll-target="#what-is-a-vae">What is a VAE?</a></li>
  <li><a href="#building-and-training-vae" id="toc-building-and-training-vae" class="nav-link" data-scroll-target="#building-and-training-vae">Building and training VAE</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">VAEs: Variational autoencoders</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Variational Autoencoders (VAEs) were introduced by <a href="https://arxiv.org/abs/1312.6114">Kingma et al</a> as an efficient method to learn posterior distributions using gradient descent. The paper describes how neural networks can be used to efficiently create models that can generate complex data by sampling from simple, known distributions. For those who are new to generative modelling contrasting VAEs with autoencoders can help highlight the key ideas.</p>
<section id="starting-simple-with-autoencoders" class="level1">
<h1>Starting simple with autoencoders</h1>
<p>If you’re familiar with building and training neural networks autoencoders are a good place to start. They’ll demonstrate why the idea of being able to sample from a distribution is important and the way they work is also fairly intuitive. As we progress it will become clear why they’re not suitable for generative modelling and we’ll discuss the changes needed to bridge the gap. Autoencoders learn low dimensional representations by passing data through an information bottleneck and try to reconstruct it at the other end. While they’re often used in representation learning tasks their structure is fairly similar to that of a VAE. To begin we’ll build one and try to use it to generate images of faces.</p>
<section id="defining-a-dataset" class="level2">
<h2 class="anchored" data-anchor-id="defining-a-dataset">Defining a dataset</h2>
<p>For this example we’ll try and generate some faces from the famous <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> dataset. The dataset contains more than 200 thousand images of celebrities. The images have also been cropped so that faces are centred, which makes the machine learning task a bit easier.</p>
<div id="fig-celeba" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_val_set_23.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_val_set_123.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_val_set_453.png" class="img-fluid figure-img"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_val_set_623.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_val_set_645.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_val_set_2645.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Examples from CelebA validation set</figcaption><p></p>
</figure>
</div>
<p>The dataset also comes with a set of attributes labels for each image for things like “Blond Hair” and “Eyeglasses”. These will be useful when we want to explore the encoded representations of the images that we generate.</p>
</section>
<section id="what-is-an-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="what-is-an-autoencoder">What is an autoencoder?</h2>
<p>You can break down an autoencoder network into two parts, the encoder and decoder. It works by passing data through the encoder, which takes the input and squeezes it down to vector that is smaller in size. This is the low dimensional bottleneck. This vector is then input to the decoder which tries to reconstruct the input image. The model is trained by minimizing the difference (more on this later) between the input image and reconstructed image. In order for this to work The output of the encoder needs to contain enough information about the input data so that the decoder network can recreate it.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/autoencoder_structure.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Autoencoder structure</figcaption>
</figure>
</div>
<p>The encoder output can be considered a low(er) dimensional vector representation of the input data. They’re often called latent or hidden vectors since they’re supposed to contain the latent attributes of the data. For our purposes though the most interesting part is the decoder. If you train an autoencoder on a collection of images then you should be able to generate new images by passing simple, low dimensional data into the decoder input!</p>
</section>
<section id="model-structure-and-training" class="level2">
<h2 class="anchored" data-anchor-id="model-structure-and-training">Model structure and training</h2>
<p>We can build the encoder and decoder layers using a series of convolutional (and transpose convolutional) layers. This is a common strategy for image based ML problems and, since there are other excellent resources describing how convolutional layers work, including the <strong>link to pytorch docs</strong>, I won’t discuss them here. All the code written for this example can be found <strong>here</strong>. In building the model for this example I took plenty of inspiration from <strong>Ref to deep learning book and the repo</strong>.</p>
<p>The nice thing is that we can abstract away most of the complexity when it comes to training. The model, with an input sample <span class="math inline">\(x\)</span> can be represented simply as <span class="math display">\[
\hat{x} = g(f(x))
\]</span> where <span class="math inline">\(\hat{x}\)</span> is the reconstructed sample and <span class="math inline">\(g\)</span> and <span class="math inline">\(f\)</span> are the decoder and encoder respectively. The latent vector <span class="math inline">\(h\)</span> is given by <span class="math display">\[
h = f(x).
\]</span> Finally, training the autoencoder means simply picking a loss function <span class="math inline">\(L\)</span> and minimising <span class="math display">\[
L(x, g(f(x))).
\]</span> All we need to do now is select a loss function. In this case <span class="math inline">\(x\)</span> is a tensor of RGB pixel intensities for each image. The values are scaled to be between 0 and 1. Using the mean squared error (MSE) between our reconstructed and original pixel values is a reasonable choice, forcing the model to try and get the correct pixel values.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Choice of loss function
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using MSE loss in image reconstruction tasks can make the model less sensitive to small amounts of gaussian blur around the target pixel values, which makes images look blurry.</p>
</div>
</div>
<p>Keeping things simple and training the model using the Adam optimiser for 10 epochs achieves reasonable results. Practically no time was spent fine tuning this model but it does pretty well. Training took about 30 minutes on a single GPU. Instead of plotting training loss curves it’s more insightful to just plot the input and output images directly.</p>
<section id="comparing-inputs-and-outputs" class="level3">
<h3 class="anchored" data-anchor-id="comparing-inputs-and-outputs">Comparing inputs and outputs</h3>
<p>A side by side comparison of input and output images of the autoencoder should give some sense of how well it’s performing.</p>
<div id="fig-aegen_in" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_test_set_7961.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_test_set_8842.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/celebA_test_set_8970.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Original images</figcaption><p></p>
</figure>
</div>
<div id="fig-aegen_out" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_gen_test_set_7961.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_gen_test_set_8842.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_gen_test_set_8970.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Autoencoder output</figcaption><p></p>
</figure>
</div>
<p>A side by side comparison shows that while a lot of detail is lost the reconstructed images do resemble the originals. At the very least they look like human faces! What’s interesting is that it’s not just blurry, some of the fine details are lost. Subtle things like where the eyes are pointing and the asymmetry of a smile. Some things like jewellery and ties are lost completely. This happens because that information doesn’t make it though the bottleneck, probably since it’s less important than other facial characteristics when it comes to reconstructing pixel values. When information is missing the decoder just substitues it with something that “sort of works” for most training examples, or misses it entirely. Its definitely possible to do better at this stage but the decoder does seem to be able to generate faces.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Making the models public
</div>
</div>
<div class="callout-body-container callout-body">
<p>If there is interest I can make the pre-trained decoder available so that you can try an generate images with it yourself.</p>
</div>
</div>
</section>
</section>
<section id="generating-new-images" class="level2">
<h2 class="anchored" data-anchor-id="generating-new-images">Generating (new) images</h2>
<p>This is where things get tricky. It should be possible to generate new faces by creating new latent vectors and passing them through the decoder. What isn’t clear however is how to generate the latent vectors. The problem of generating faces has now turned into the problem of generating latent vectors. This is the issue that VAEs address and is a good way to understand the underlying assumptions.</p>
<p>Before we explore that however, we can see what happens if we try and make faces by taking an educated guess at what reasonable latent vectors look like. Remember, the goal is generate faces that are realistic and human looking, but distinct from those that are in the dataset. We can try and be clever and encode a set of images from the test set, take the mean and standard deviation values of the latent vectors generated and, in a bit of a hand-wavey attempt to generate vectors from the same distribution as the encoded sample, generate new vectors by sampling from a normal distribution with the same parameters.</p>
<div id="fig-celeba" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_random_gen_0.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_random_gen_1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_random_gen_2.png" class="img-fluid figure-img"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_random_gen_3.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_random_gen_4.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/ae_random_gen_5.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Passing random latent vectors through the decoder generates these images.</figcaption><p></p>
</figure>
</div>
<p>Turns out this doesn’t work so well. If you squint at some of these image you can just about see a tortured face peering out from under the random blobs of colour, but nothing like the image that were generated above. This is the problem with using models like autoencoders to generate new data. There are no constraints on what values the latent vectors take, so when it comes to trying to generate new images you’re stuck.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Going a step further and generating vectors by using the mean and standard deviation of each dimension independently generates slightly more realistic looking images, but similar problems are observed.</p>
</div>
</div>
</section>
</section>
<section id="vaes-to-the-rescue" class="level1">
<h1>VAEs to the rescue</h1>
<p>Now that you’re convinced that just using autoencoders is insufficient we can explore the changes introduced by VAEs. Instead of mapping individual data points to points in latent space VAEs learn latent space distributions that are likely to generate realistic looking data.</p>
<section id="thinking-in-terms-of-distributions" class="level2">
<h2 class="anchored" data-anchor-id="thinking-in-terms-of-distributions">Thinking in terms of distributions</h2>
<p>Instead of optimising encoder and decoder functions consider a VAE as a latent variable model. VAEs address the problem of sampling the latent vectors directly by constraining them to be drawn from a known distribution, modelling the join probability of the latent variables <span class="math inline">\(z\)</span> and the data <span class="math inline">\(x\)</span>. This is the equivalent of putting a prior on the <span class="math inline">\(z\)</span> vectors in a Bayesian framework. Typically we choose this distribution to be a isotropic multidimensional Gaussian, assuming no correlation between any of the dimensions of the latent vector: <span class="math display">\[
P(z) \sim \mathcal{N}(0, I).
\]</span> The choice of distribution is just an assumptions but has some useful properties that we’ll see later. This change in modelling assumptions has a significant impact on how VAEs are trained and it becomes clear that they’re actually only superficially similar to autoencoders.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why a Gaussian?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A natural question to ask is whether the choice of prior distribution constrains the model outputs in anyway. The good news is that it doesn’t, any <span class="math inline">\(d\)</span> dimensional distribution can be generated by sampling <span class="math inline">\(d\)</span> variables from a normal distribution and transforming them with a sufficiently complex function. This is how inverse-transform sampling works, as discussed on <a href="../generative_ml/index.html">this page</a>.</p>
</div>
</div>
</div>
<p>We can write down the model as the join probability of the latent vectors and the data, <span class="math inline">\(P(x, z)\)</span>. Using the chain rule of probability we can write this out in a way that better defines the generative process: <span class="math display">\[
P(x, z) = P(x | z)P(z)
\]</span> where the process is</p>
<ol type="1">
<li>Sample a latent vector <span class="math inline">\(z \sim P(z)\)</span></li>
<li>Generate a data point <span class="math inline">\(x \sim P(x | z)\)</span>.</li>
</ol>
<p>It is helpful to rewrite <span class="math inline">\(P(x | z)\)</span> as <span class="math inline">\(P(x | z; \theta)\)</span>, where <span class="math inline">\(\theta\)</span> represents the parameters of the distribution <span class="math inline">\(P_{x|z}\)</span>. What we want to optimise (maximise in this case), with respect to <span class="math inline">\(\theta\)</span>, is the probability of each training example <span class="math inline">\(x\)</span> under the generative process <span class="math display">\[
\begin{aligned}
P(x) &amp; = \int P(x | z; \theta)P(z) dz. \\
&amp; = \mathbb{E}_{z \sim P(z)}P(x | z; \theta)
\end{aligned}
\]</span> We have already chosen how to define <span class="math inline">\(P(z)\)</span> but we still need to compute the integral over <span class="math inline">\(z\)</span>. There are some key issues that make this quite difficult.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Kullback–Leibler (KL) divergence
</div>
</div>
<div class="callout-body-container callout-body">
<p>The KL divergence is defined as <span class="math display">\[
\begin{aligned}
\mathcal{D}(P \parallel Q) &amp;= \int p(x) \log \left( \frac{p(x)}{q(x)} \right) dx \\
&amp;= \mathbb{E}_{p(x)} \left[ \frac{p(x)}{q(x)} \right]
\end{aligned}
\]</span> and is used as a measure to describe how one probability distribution differs from another.</p>
</div>
</div>
</section>
<section id="what-is-a-vae" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-vae">What is a VAE?</h2>
<p>In principle we can maximise <span class="math inline">\(P(x)\)</span> directly by sampling <span class="math inline">\(n\)</span> values of <span class="math inline">\(z\)</span> and then computing <span class="math inline">\(P(x) \approx \frac{1}{n} \sum_i P(x | z_i; \theta)\)</span>. The problem here is just how many samples it would take. If you consider all the possible configurations of the pixels in the images <span class="math inline">\(x\)</span> its obvious that for the vast majority of the time <span class="math inline">\(P(x|z; \theta)\)</span> will be almost zero and sampling the corresponding <span class="math inline">\(z\)</span> values will not help improve our estimates of <span class="math inline">\(P(x|z; \theta)\)</span>. This makes using traditional inference techniques such as Markov Chain Monte Carlo (MCMC) difficult, even before you consider the number of parameters. The discussion in <a href="https://arxiv.org/abs/1606.05908">this</a> excellent introduction goes into more detail.</p>
<p>What makes VAEs more efficient is that during training we try and sample values of <span class="math inline">\(z\)</span> that are likely to produce <span class="math inline">\(x\)</span>. This is done by introducing a new function <span class="math inline">\(Q( z | x)\)</span>, which takes a value of <span class="math inline">\(x\)</span> and returns a distribution over <span class="math inline">\(z\)</span> values which are likely to produce <span class="math inline">\(x\)</span>. Where the generative model we’ve describe is analogous to the decoder part of the autoencoder this part is analogous to the encoder.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is a bit of notation overload here, which I have coped from <a href="https://arxiv.org/abs/1606.05908">Doersch</a>. I’m using P(…) to sometimes refer to generic probability distributions and sometimes specific functions. Hopefully with context its pretty clear in this example.</p>
</div>
</div>
<p>Since we now sample <span class="math inline">\(z \sim Q(z | x)\)</span> our definition for <span class="math inline">\(P(x)\)</span> becomes <span class="math display">\[
\begin{aligned}
P(x) &amp;= \int P(x | z; \theta)Q(z | x) dz \\
&amp;= \mathbb{E}_{z \sim Q}P(x | z; \theta).
\end{aligned}
\]</span> Our goal is now to re-write this in terms of an expression which we can evaluate easily and therefore maximise. We do this by defining the ELBO (evidence lower bound) and showing that maximising the ELBO also maximises <span class="math inline">\(P(x)\)</span>. (Or, <span class="math inline">\(\ln P(x)\)</span>, since the logarithm is a monotonic function.)</p>
<section id="deriving-the-elbo" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-elbo">Deriving the ELBO</h3>
Starting from the definition of <span class="math inline">\(P(x)\)</span> and taking the natural logarithm,
<span class="math display">\[\begin{aligned}
\ln P(x) &amp;= \ln \left[  \mathbb{E}_{z \sim Q} P(x)\right] = \mathbb{E}_{z \sim Q} \ln  P(x)  &amp;(P(x)~\text{is independent of}~z) \\

&amp; =  \mathbb{E}_{z \sim Q} \left[ \ln \frac{P(x | z)P(z)}{P(z | x)} \right] &amp; (\text{Bayes' rule}) \\

&amp; = \mathbb{E}_{z \sim Q} \left[ \ln \frac{P(x | z)P(z)}{P(z | x)} \frac{Q(z | x)}{Q(z | x)} \right] &amp; (\text{multiply by}~1) \\

&amp; = \mathbb{E}_{z \sim Q} \ln P(x | z)
- \mathbb{E}_{z \sim Q} \left[ \ln \frac{Q(z | x)}{P(z)} \right]  
+ \mathbb{E}_{z \sim Q} \left[ \ln \frac{Q(z | x)}{P(z | x)} \right] \\\\

\ln P(x) &amp;= \mathbb{E}_{z \sim Q} \ln P(x | z) - \mathcal{D}(Q(z | x) \parallel P(z))
         +
\underbrace{\mathcal{D}(Q(z | x) \parallel P(z | x))}_{\text{intractable},~\geq 0}.
\end{aligned}\]</span>
<p>It’s possible to express the quantity we want to maximise in terms of several terms one of which is The KL divergence of <span class="math inline">\(Q\)</span> with an intractable term <span class="math inline">\(P(z | x)\)</span>. The critical thing consider here is that the KL divergence of any two distributions is always <span class="math inline">\(\geq 0\)</span>. This allows us to write</p>
<span class="math display">\[\begin{aligned}
\ln P(x) &amp;\geq \underbrace{
    \underbrace{
        \mathbb{E}_{z \sim Q} \ln P(x | z)
        }_\text{reconstruction error}
        -
        \underbrace{
            \mathcal{D}(Q(z | x) \parallel P(z)).
        }_\text{fixes the latent}
}_\text{ELBO}
\end{aligned}\]</span>
<p>The ELBO consists of two terms</p>
<ul>
<li>One that represents the reconstruction error</li>
<li>The (negative) KL divergence between <span class="math inline">\(Q\)</span> and the desired distribution of <span class="math inline">\(z\)</span>, <span class="math inline">\(P(z)\)</span>. This adds a larger penalty for versions of <span class="math inline">\(Q\)</span> that generate <span class="math inline">\(z\)</span> distributions that deviate from <span class="math inline">\(P(z)\)</span>.</li>
</ul>
<p>Since <span class="math inline">\(P(z)\)</span> is bound to be greater or equal to the ELBO, maximising the ELBO maximises the quantity that we care about.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alternate derivations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There are several methods to derive the ELBO. One method relies on Jensen’s inequality while another starts with KL divergence between <span class="math inline">\(Q(z|x)\)</span> and the unknown distribution <span class="math inline">\(P(z|x)\)</span>. The method I’ve chosen starts from the definition of <span class="math inline">\(P(x)\)</span> given above.</p>
</div>
</div>
</div>
</section>
</section>
<section id="building-and-training-vae" class="level2">
<h2 class="anchored" data-anchor-id="building-and-training-vae">Building and training VAE</h2>
<p>Now that we’ve developed a theoretical understanding of that a VAE is, it should be possible to map the autoencoder architecture we’ve used already into one. The encoder does the job of <span class="math inline">\(Q(z | x)\)</span>, mapping images to latent vectors. The main difference however is that the encoder should generate a distribution of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span> rather than a scalar vector. The decoder in turn represents <span class="math inline">\(P(x | z).\)</span> This introduces a new challenge. How can we sample from a parametrised distribution while training a neural network via gradient descent? We need to be able to calculate the gradient of the loss with respect to all of the trainable parameters in the network.</p>
<section id="the-reparametrisation-trick" class="level4">
<h4 class="anchored" data-anchor-id="the-reparametrisation-trick">The reparametrisation trick</h4>
<p>Turns out that there is an elegant solution. The reparametrisation trick separates the stochastic part from the trainable parameters of the distribution. The encoder needs to generate samples drawn from a multivariate Gaussian distribution, parametrised by mean vector <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma\)</span>. This can be done by modifying the autoencoder encoder to output two vectors, one that represents the mean <span class="math inline">\(\mu\)</span> and one for (the log of) the variance <span class="math inline">\(\sigma\)</span>. The stochastic encoder output is then created by sampling a vector from the standard normal distribution and then shifting and scaling appropriately <span class="math display">\[
Q(z | x) = \mu + ( \mathcal{N}(0, 1) \cdot \sigma  ).
\]</span> During training we can therefore still calculate gradients with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. We could sample values probabilistically for the pixel densities but this isn’t necessary in order to calculate the loss and typically isn’t done.</p>
</section>
<section id="defining-the-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-loss-function">defining the loss function</h3>
<p>The first term of the loss function, based on the ELBO, represents the likelihood of generating the target image <span class="math inline">\(x\)</span> and can be interpreted as the reconstruction error. In the first example we used the MSE loss for our reconstruction error. Under the assumption that the pixel values are drawn from normal distributions the MSE loss can be interpreted as a likelihood, so that part can remain the same. Defining the second term can be defined relatively easily since we’ve fixed both <span class="math inline">\(P(z)\)</span> and <span class="math inline">\(Q(z | x)\)</span> to be a multivariate Guassians. The loss function then becomes</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The <span class="math inline">\(\beta\)</span>-VAE
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>blah</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/vae/VAE_latent_distributions.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The distribution of latent dimension vectors as encoded from the validation set</figcaption>
</figure>
</div>
<div id="fig-celeba" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/vae_random_gen_3.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/vae_random_gen_1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/vae_random_gen_2.png" class="img-fluid figure-img"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/vae_random_gen_9.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/vae_random_gen_13.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/vae/vae_random_gen_12.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Passing random latent vectors through the VAE generates somewhat realistic looking faces.</figcaption><p></p>
</figure>
</div>
<p>% mention this after adding something about P(z|x) It might be a good time to pause here before we discuss how we can optimise this and think about why this formulation helps us generate new data. During training</p>
<p>Not only will be now be able to sample latent vectors from</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A PyTorch example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>It’s possible to make relatively small modifications to an autoencoder implemented in PyTorch and turn it into a VAE.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VAE(AutoEncoder):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""construct a VAE by using the basic same skeleton</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    as the AutoEncoder, but by adding and overwriting a few things</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        latent_dim,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        input_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        encoder_channel_out_sizes<span class="op">=</span>[<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        input_image_h_w<span class="op">=</span>(<span class="dv">218</span>, <span class="dv">178</span>),</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>            latent_dim,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            input_channels,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            encoder_channel_out_sizes,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            input_image_h_w,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add another linear layer for the log variance</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_linear_logvar <span class="op">=</span> nn.Linear(<span class="va">self</span>.linear_layer_size, <span class="va">self</span>.latent_dim)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reparameterise(mu, logvar):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implement the reparameterisation trick"""</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># scale the log variance to std</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> logvar)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sample from a standard noem</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        normal_noise <span class="op">=</span> torch.randn_like(std)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu <span class="op">+</span> (std <span class="op">*</span> normal_noise)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, <span class="bu">input</span>: torch.Tensor):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Helper function making it easier to encode</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">        images</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.encoder(<span class="bu">input</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> <span class="va">self</span>.encoder_linear_mean(x)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        logvar <span class="op">=</span> <span class="va">self</span>.encoder_linear_logvar(x)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.reparameterise(mu, logvar), mu, logvar</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        encoded, mu, logvar <span class="op">=</span> <span class="va">self</span>.encode(<span class="bu">input</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decode(encoded), mu, logvar</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss_function(<span class="va">self</span>, <span class="bu">input</span>, target, mu, logvar):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># summed over each element in the latent space</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the mean is taken over training examples</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        kld_loss <span class="op">=</span> torch.mean(</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="dv">1</span> <span class="op">+</span> logvar <span class="op">-</span> mu<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> logvar.exp(), dim<span class="op">=</span><span class="dv">1</span>), dim<span class="op">=</span><span class="dv">0</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        kld_loss <span class="op">=</span> kld_loss <span class="op">*</span> <span class="fl">1e-5</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        reconstruction_loss <span class="op">=</span> nn.functional.mse_loss(<span class="bu">input</span>, target)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            reconstruction_loss <span class="op">+</span> kld_loss,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            reconstruction_loss.detach(),</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            kld_loss.detach(),</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Intuitively</p>
<blockquote class="blockquote">
<p>I remember asking a researcher at my old job exactly why VAEs made the assumption that elements in the latent space were normally distributed and the response was “that’s just the assumption of the model. At the time it did not help.</p>
</blockquote>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references">References</h4>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>